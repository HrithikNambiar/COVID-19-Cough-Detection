{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CCS_svm.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HrithikNambiar/COVID-19-Cough-Detection/blob/main/CCS_svm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3ukC6L6bxKs",
        "outputId": "364b28b8-7f7e-4e66-f601-719edc0d0193"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZweikFHqcmC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8e42829-7531-4fd6-cad8-1d7e973a96fc"
      },
      "source": [
        "!pip install liac-arff\n",
        "!pip install -U PyYAML"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting liac-arff\n",
            "  Downloading https://files.pythonhosted.org/packages/6e/43/73944aa5ad2b3185c0f0ba0ee6f73277f2eb51782ca6ccf3e6793caf209a/liac-arff-2.5.0.tar.gz\n",
            "Building wheels for collected packages: liac-arff\n",
            "  Building wheel for liac-arff (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for liac-arff: filename=liac_arff-2.5.0-cp37-none-any.whl size=11732 sha256=1e32e3870eaa5290ff7b5126eb183a1e803fafc90cd080acaebaaff86187b6b6\n",
            "  Stored in directory: /root/.cache/pip/wheels/77/8d/b4/8bfce5beea9a3496cc15b24961876adb7b6e2912ff09164179\n",
            "Successfully built liac-arff\n",
            "Installing collected packages: liac-arff\n",
            "Successfully installed liac-arff-2.5.0\n",
            "Collecting PyYAML\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7a/a5/393c087efdc78091afa2af9f1378762f9821c9c1d7a22c5753fb5ac5f97a/PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636kB)\n",
            "\u001b[K     |████████████████████████████████| 645kB 8.6MB/s \n",
            "\u001b[?25hInstalling collected packages: PyYAML\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed PyYAML-5.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qgW5uRYqpmr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4959a1e2-9c08-464c-fbd0-5d87dc1100a2"
      },
      "source": [
        "cd '/content/drive/MyDrive/ComParE2021'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1a6R2J7y9rkZWY_SyloMCwmm8qdQAliCz/ComParE2021\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "et5NG6i2PXxr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f01c78df-2941-413d-948a-775848741446"
      },
      "source": [
        "from imblearn.pipeline import Pipeline\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.svm import LinearSVC, SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.base import clone\n",
        "# from sklearn.pipeline import Pipeline\n",
        "from sklearn.utils import resample\n",
        "from sklearn.model_selection import PredefinedSplit, GridSearchCV\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix, recall_score, make_scorer\n",
        "from joblib import Parallel, delayed\n",
        "import pandas as pd\n",
        "import scipy\n",
        "import os, yaml\n",
        "import json\n",
        "import sys\n",
        "import arff\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from glob import glob"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCrfPkIqPiat"
      },
      "source": [
        "FEATURE_BASE='./features_2'  #path to the directory containing features\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "GRID = [\n",
        "    {'scaler': [StandardScaler(), None],\n",
        "     'estimator': [SVC(probability=True )],\n",
        "    #  'estimator__loss': ['squared_hinge'],\n",
        "     'estimator__kernel' : ['linear', 'rbf'],\n",
        "     'estimator__C': np.logspace(-1, -5, num=5),\n",
        "     'estimator__class_weight': ['balanced', None],\n",
        "     'estimator__max_iter': [10000]\n",
        "     }\n",
        "]\n",
        "\n",
        "PIPELINE = Pipeline([('scaler', None), ('sampling', RandomUnderSampler()), ('estimator', SVC())])\n",
        "\n",
        "# GRID = [\n",
        "#     {'scaler': [StandardScaler(), MinMaxScaler(), None],\n",
        "#      'estimator': [RandomForestClassifier(random_state=RANDOM_SEED)],\n",
        "#     #  'estimator__n_estimators': [200, 400, 600, 800, 1000],\n",
        "#      }\n",
        "# ]\n",
        "\n",
        "# PIPELINE = Pipeline([('scaler', None), ('sampling', SMOTE()), ('estimator', RandomForestClassifier())])\n",
        "\n",
        "# clf1 = SVC(random_state=RANDOM_SEED, probability=True)\n",
        "# clf2 = RandomForestClassifier(random_state=RANDOM_SEED)\n",
        "# clf3 = XGBClassifier(random_state=RANDOM_SEED)\n",
        "# clf4 = MLPClassifier(hidden_layer_sizes=(256, 256, 256,))\n",
        "\n",
        "# eclf = VotingClassifier(estimators=[('svm', clf1), ('rf', clf2), ('clf3', clf3), ('nn', clf4)],voting='soft')\n",
        "\n",
        "# GRID = [\n",
        "#     {'scaler': [MinMaxScaler()],\n",
        "#      'estimator': [eclf],\n",
        "#     #  'estimator__n_estimators': [200, 400, 600, 800, 1000],\n",
        "#      }\n",
        "# ]\n",
        "\n",
        "# PIPELINE = Pipeline([ ('scaler', None), ('sampling', RandomUnderSampler()), ('estimator', eclf)])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chfi_hOoPpvT"
      },
      "source": [
        "def make_dict_json_serializable(meta_dict: dict) -> dict:\n",
        "    cleaned_meta_dict = meta_dict.copy()\n",
        "    for key in cleaned_meta_dict:\n",
        "        if type(cleaned_meta_dict[key]) not in [str, float, int, np.float]:\n",
        "            cleaned_meta_dict[key] = str(cleaned_meta_dict[key])\n",
        "    return cleaned_meta_dict\n",
        "\n",
        "def bootstrap(best_estimator, X, y, test_X, test_y, random_state):\n",
        "    estimator = clone(best_estimator)\n",
        "    sample_X, sample_y = resample(X, y, random_state=random_state)\n",
        "    estimator.fit(sample_X, sample_y)\n",
        "    _preds = estimator.predict(test_X)\n",
        "    return recall_score(test_y, _preds, average='macro')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9p0puIozETK"
      },
      "source": [
        "def run_predictions_devel(model, feature_folder, results_folder): \n",
        "  devel_file = glob(os.path.join(dirpath, 'val.*'))[0]\n",
        "  if \"auDeep\" in dirpath:\n",
        "      label_index = -2  #because last column of auDeep feature is the label\n",
        "  else:\n",
        "      label_index = -1\n",
        "\n",
        "  if devel_file.endswith('.arff'):\n",
        "    with open(devel_file) as f:\n",
        "        arff_data = arff.load(f)\n",
        "        devel_names = np.array(arff_data['data'])[:, 0]\n",
        "        devel_X = np.array(arff_data['data'])[:, 1:label_index].astype(np.float32)\n",
        "        devel_y = np.array(arff_data['data'])[:, label_index].astype(str)\n",
        "  else:\n",
        "    devel_df = pd.read_csv(devel_file)\n",
        "    devel_names = devel_df.values[:, 0]\n",
        "    devel_X = devel_df.values[:, 1:label_index].astype(np.float32)\n",
        "    devel_y = devel_df.values[:, label_index].astype(str)\n",
        "  \n",
        "  preds = model.predict(devel_X)\n",
        "\n",
        "  prob = model.predict_proba(devel_X)\n",
        "  pos_prob = prob[:, 1]\n",
        "\n",
        "  df_predictions = pd.DataFrame({'filename': devel_names.tolist(), 'prediction': preds.tolist(), 'pos_prob': pos_prob.tolist()})\n",
        "  df_predictions.to_csv(os.path.join('/content/', 'devel.predictions.csv'), index=False)\n",
        "\n",
        "  df = pd.read_csv(\"/content/devel.predictions.csv\")\n",
        "  temp_dict = {}\n",
        "\n",
        "  for i in range(len(df)):\n",
        "    f = df['filename'][i][:9]+'.wav'\n",
        "    c = df['prediction'][i]\n",
        "    pos = df['pos_prob'][i]\n",
        "    if f in temp_dict.keys():\n",
        "      temp_dict[f].append([c, pos])\n",
        "    else:\n",
        "      temp_dict[f] = []\n",
        "      temp_dict[f].append([c, pos])\n",
        "\n",
        "  fnames = []\n",
        "  preds = []\n",
        "  for i in temp_dict.keys():\n",
        "    cnt_pos = 0\n",
        "    cnt_neg = 0\n",
        "    probs = []\n",
        "    for i in temp_dict[i]:\n",
        "      if i[0] == 'positive':\n",
        "        cnt_pos += 1\n",
        "      else:\n",
        "        cnt_neg += 1\n",
        "      probs.append(i[1])\n",
        "      \n",
        "    if cnt_pos > cnt_neg: \n",
        "      preds.append('positive')\n",
        "    elif cnt_neg > cnt_pos:\n",
        "      preds.append('negative')\n",
        "    else:\n",
        "      temp = np.mean(probs)\n",
        "      if temp >= 0.5:\n",
        "        preds.append('positive')\n",
        "      else:\n",
        "        preds.append('negative')\n",
        "    fnames.append(i)\n",
        "\n",
        "  gt_df = pd.read_csv('./dist/lab/devel.csv')\n",
        "  gt_preds = list(gt_df['label'])\n",
        "  gt_df.head()\n",
        "\n",
        "  uar = recall_score(gt_preds, preds, average='macro')\n",
        "  print(uar)\n",
        "\n",
        "  cm = confusion_matrix(gt_preds, preds)\n",
        "  print(cm)\n",
        "\n",
        "  df_predictions = pd.DataFrame({'filename': fnames, 'prediction': preds})\n",
        "  df_predictions.to_csv(os.path.join(results_folder, 'devel.predictions.csv'), index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVbvN4KEPqmM"
      },
      "source": [
        "def run_svm(feature_folder, results_folder, params):\n",
        "    train_file = glob(os.path.join(feature_folder, 'train.*'))[0] \n",
        "    # devel_file = glob(os.path.join(feature_folder, 'devel.*'))[0]\n",
        "    devel_file = glob(os.path.join(feature_folder, 'val.*'))[0]\n",
        "    test_file = glob(os.path.join(feature_folder, 'test.*'))[0]\n",
        "    if \"auDeep\" in feature_folder:\n",
        "        label_index = -2  #because last column of auDeep feature is the label\n",
        "    else:\n",
        "        label_index = -1\n",
        "    if train_file.endswith('.arff'):\n",
        "        with open(train_file) as f:\n",
        "            arff_data = arff.load(f)\n",
        "            train_X = np.array(arff_data['data'])[:, 1:label_index].astype(np.float32)\n",
        "            train_y = np.array(arff_data['data'])[:, label_index].astype(str)\n",
        "            feature_names = [attribute[0] for attribute in arff_data['attributes'][1:label_index]]\n",
        "\n",
        "        with open(devel_file) as f:\n",
        "            arff_data = arff.load(f)\n",
        "            devel_names = np.array(arff_data['data'])[:, 0]\n",
        "            devel_X = np.array(arff_data['data'])[:, 1:label_index].astype(np.float32)\n",
        "            devel_y = np.array(arff_data['data'])[:, label_index].astype(str)\n",
        "\n",
        "        with open(test_file) as f:\n",
        "            arff_data = arff.load(f)\n",
        "            test_names = np.array(arff_data['data'])[:, 0]\n",
        "            test_X = np.array(arff_data['data'])[:, 1:label_index].astype(np.float32)\n",
        "            test_y = np.array(arff_data['data'])[:, label_index].astype(str)\n",
        "\n",
        "    else:\n",
        "        train_df = pd.read_csv(train_file)\n",
        "        devel_df = pd.read_csv(devel_file)\n",
        "        test_df = pd.read_csv(test_file)\n",
        "        feature_names = list(train_df.columns)[1:label_index]  #label_index of AuDeep is -2, since last column is the label\n",
        "        \n",
        "        train_X = train_df.values[:, 1:label_index].astype(np.float32)\n",
        "        train_y = train_df.values[:, label_index].astype(str)\n",
        "\n",
        "        devel_names = devel_df.values[:, 0]\n",
        "        devel_X = devel_df.values[:, 1:label_index].astype(np.float32)\n",
        "        devel_y = devel_df.values[:, label_index].astype(str)\n",
        "\n",
        "        test_names = test_df.values[:, 0]\n",
        "        test_X = test_df.values[:, 1:label_index].astype(np.float32)\n",
        "        test_y = test_df.values[:, label_index].astype(str)\n",
        "        \n",
        "    num_train = train_X.shape[0]\n",
        "    num_devel = devel_X.shape[0]\n",
        "    split_indices = np.repeat([-1, 0], [num_train, num_devel])\n",
        "    split = PredefinedSplit(split_indices)\n",
        "    \n",
        "    X = np.append(train_X, devel_X, axis=0) #number of training data\n",
        "    y = np.append(train_y, devel_y, axis=0) #number of val data\n",
        "    \n",
        "    grid_search = GridSearchCV(estimator=PIPELINE, param_grid=GRID, \n",
        "                                scoring=make_scorer(recall_score, average='macro'), \n",
        "                                n_jobs=-1, cv=split, refit=True, verbose=1, \n",
        "                                return_train_score=False)\n",
        "    \n",
        "    # fit on data. train -> devel first, then train+devel implicit\n",
        "    grid_search.fit(X, y)\n",
        "    best_estimator = grid_search.best_estimator_\n",
        "    \n",
        "    # fit clone of best estimator on train again for devel predictions\n",
        "    estimator = clone(best_estimator, safe=False)\n",
        "    estimator.fit(train_X, train_y)\n",
        "    preds = estimator.predict(devel_X)\n",
        "\n",
        "    metrics = {'dev': {}, 'test': {}}\n",
        "\n",
        "    # devel metrics\n",
        "    run_predictions_devel(estimator, feature_folder, results_folder)\n",
        "    # uar = recall_score(devel_y, preds, average='macro')\n",
        "    # cm = confusion_matrix(devel_y, preds)\n",
        "    # print(f'UAR: {uar}\\n{classification_report(devel_y, preds)}\\n\\nConfusion Matrix:\\n\\n{cm}') \n",
        "    # metrics['dev']['uar'] = uar\n",
        "    # metrics['dev']['cm'] = cm.tolist()\n",
        "    # metrics['params'] = make_dict_json_serializable(grid_search.best_params_)\n",
        "\n",
        "    # df_predictions = pd.DataFrame({'filename': devel_names.tolist(), 'prediction': preds.tolist()})\n",
        "    # df_predictions.to_csv(os.path.join(results_folder, 'devel.predictions.csv'), index=False)\n",
        "\n",
        "    pd.DataFrame(grid_search.cv_results_).to_csv(\n",
        "        os.path.join(results_folder, 'grid_search.csv'), index=False)\n",
        "\n",
        "    # test metrics\n",
        "    # print(f'Generating test predictions for optimised parameters {metrics[\"params\"]}')\n",
        "    preds = best_estimator.predict(test_X)\n",
        "    if len(set(test_y)) > 1: # test labels exist\n",
        "        uar = recall_score(test_y, preds, average='macro')\n",
        "        cm = confusion_matrix(test_y, preds)\n",
        "        metrics['test']['uar'] = uar\n",
        "        metrics['test']['cm'] = cm.tolist()\n",
        "        print(f'UAR: {uar}\\n{classification_report(test_y, preds)}\\n\\nConfusion Matrix:\\n\\n{cm}') \n",
        "\n",
        "        print('Computing CI...')\n",
        "        uars = list(Parallel(n_jobs=-1, verbose=10)(delayed(bootstrap)(best_estimator, X, y, test_X, test_y, i) for i in range(params['bootstrap_iterations'])))\n",
        "        ci_low, ci_high = scipy.stats.t.interval(params['ci_interval'], len(uars)-1, loc=np.mean(uars), scale=scipy.stats.sem(uars))\n",
        "        metrics['test']['ci_low'] = ci_low\n",
        "        metrics['test']['ci_high'] = ci_high\n",
        "        metrics['test']['uar_mean'] = np.mean(uars)\n",
        "        \n",
        "\n",
        "    df_predictions = pd.DataFrame({'filename': test_names.tolist(), 'prediction': preds.tolist()})\n",
        "    df_predictions.to_csv(os.path.join(results_folder, 'test.predictions.csv'), index=False)\n",
        "\n",
        "    with open(os.path.join(results_folder, 'metrics.json'), 'w') as f:\n",
        "        json.dump(metrics, f)\n",
        "    \n",
        "    return estimator\n",
        "    # feature ranking\n",
        "    # labels = sorted(set(devel_y))\n",
        "    # if len(set(devel_y)) < 3:\n",
        "    #     df = pd.DataFrame(data={'feature': feature_names, f'importance_for_{labels[1]}': best_estimator.named_steps.estimator.coef_[0, :]})\n",
        "    # else:\n",
        "    #     df = pd.DataFrame(data={**{'feature': feature_names}, **{f'importance_for_{labels[i]}': best_estimator.named_steps.estimator.coef_[i, :] for i in range(len(labels))}})\n",
        "    # df.to_csv(os.path.join(results_folder, 'ranking.csv'), index=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5I9wEt6PyRD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b10791ba-82d4-4429-b96f-c219d0b7a19c"
      },
      "source": [
        "params = {}\n",
        "with open('params.yaml') as f:\n",
        "    params = yaml.load(f, Loader=yaml.FullLoader)\n",
        "    params = params['svm']\n",
        "\n",
        "feature_type = 'deepspectrum'\n",
        "feature_base = f'{FEATURE_BASE}/{feature_type}'\n",
        "result_base = f'./results/{feature_type}'\n",
        "for dirpath, dirnames, filenames in os.walk(feature_base):\n",
        "    if not dirnames:\n",
        "        file_extension = os.path.splitext(filenames[0])[1]\n",
        "        result_dir = os.path.join(result_base, os.path.relpath(dirpath, start=feature_base))\n",
        "        os.makedirs(result_dir, exist_ok=True)\n",
        "        model = run_svm(dirpath, result_dir, params)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 40 candidates, totalling 40 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:  9.2min finished\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:231: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:231: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.5022199453551912\n",
            "[[119  64]\n",
            " [ 31  17]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_c7E_h3B1C1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}